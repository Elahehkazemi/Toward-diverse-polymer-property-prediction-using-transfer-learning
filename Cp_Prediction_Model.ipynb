{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import math as math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from numpy.random import seed\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression, make_blobs\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing, metrics, model_selection\n",
    "\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import Huber\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset, PCA analysis, and spilt them:\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "xls1 = pd.ExcelFile('polymer properties.xls')\n",
    "\n",
    "file_part1  = pd.DataFrame()\n",
    "\n",
    "num =0 \n",
    "while num < 109:\n",
    "    part1 = pd.read_excel(xls1,'descriptors.xls_'+ str(num))\n",
    "    file_part1 = pd.concat([file_part1,part1], axis=1)\n",
    "    num +=1\n",
    "\n",
    "count =0\n",
    "\n",
    "\n",
    "dataset = file_part1.values\n",
    "\n",
    "#name\n",
    "A  = dataset [:, 1]\n",
    "#X is the value of descriptors\n",
    "X = dataset [:,3:]\n",
    "Y = dataset [:,2]\n",
    "\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate a PCA object with 6 components\n",
    "pca = PCA(n_components=13)\n",
    "\n",
    "# Fit the PCA model to your scaled dataset\n",
    "X_pca = pca.fit_transform(X_scale)\n",
    "\n",
    "\n",
    "\n",
    "#CP:\n",
    "\n",
    "X_cp= X_pca[:123]\n",
    "Y_cp=Y[:123]\n",
    "\n",
    "X_cv= X_pca[123:133]\n",
    "Y_cv=Y[123:133]\n",
    "print(Y_cv)\n",
    "\n",
    "X_flexural= X_pca[133:146]\n",
    "Y_flexural=Y[133:146]\n",
    "\n",
    "print(Y_flexural)\n",
    "\n",
    "\n",
    "X_shear= X_pca[146:164]\n",
    "Y_shear=Y[146:164]\n",
    "\n",
    "print(Y_shear)\n",
    "\n",
    "X_dynamic= X_pca[164:]\n",
    "Y_dynamic=Y[164:]\n",
    "print(Y_dynamic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ebfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model with MAE loss function to predict Cp\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "X_cp_train, X_cp_test, Y_cp_train, Y_cp_test = train_test_split(X_cp, Y_cp, test_size=0.35, random_state=70)\n",
    "\n",
    "X_cp_train = np.array(X_cp_train).astype(\"float\")\n",
    "Y_cp_train = np.array(Y_cp_train).astype(\"float\")\n",
    "X_cp_test = np.array(X_cp_test).astype(\"float\")\n",
    "Y_cp_test = np.array(Y_cp_test).astype(\"float\")\n",
    "\n",
    "tf.keras.utils.set_random_seed(20)\n",
    "\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 10)  # Number of layers\n",
    "    units = trial.suggest_int('units', 32, 1024, step=32)  # Number of neurons in each layer\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)  # Dropout rate\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)  # Learning rate\n",
    "\n",
    "    # Build model based on suggested parameters\n",
    "    model = keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units, activation='relu', input_dim=13))\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))  # Output layer\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mae', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_cp_train, Y_cp_train, validation_split=0.3, epochs=trial.suggest_int('epochs', 50, 300), batch_size=10, verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    Y_cp_pred_test = model.predict(X_cp_test)\n",
    "\n",
    "    # Compute mean absolute error (MAE) as the objective metric\n",
    "    mae_test = mean_absolute_error(Y_cp_test, Y_cp_pred_test)\n",
    "    \n",
    "    return mae_test  # Optuna will minimize this value\n",
    "\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best trial results\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value (MAE):\", trial.value)\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Plot results of the best model\n",
    "best_model = keras.Sequential()\n",
    "best_model.add(tf.keras.layers.Dense(trial.params['units'], activation='relu', input_dim=13))\n",
    "\n",
    "for i in range(trial.params['num_layers']):\n",
    "    best_model.add(tf.keras.layers.Dense(trial.params['units'], activation='relu'))\n",
    "    best_model.add(tf.keras.layers.Dropout(trial.params['dropout_rate']))\n",
    "\n",
    "best_model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the best model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=trial.params['learning_rate'])\n",
    "best_model.compile(optimizer=optimizer, loss='mae', metrics=['accuracy'])\n",
    "\n",
    "best_model.fit(X_cp_train, Y_cp_train, validation_split=0.3, epochs=trial.params['epochs'], batch_size=10)\n",
    "\n",
    "Y_cp_pred_train = best_model.predict(X_cp_train)\n",
    "Y_cp_pred_test = best_model.predict(X_cp_test)\n",
    "\n",
    "# R2 Score\n",
    "R2train = r2_score(Y_cp_train, Y_cp_pred_train)\n",
    "R2test = r2_score(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('R2 Score on Train set:', R2train)\n",
    "print('R2 Score on Test set:', R2test)\n",
    "\n",
    "# Plot actual vs predicted\n",
    "plt.figure(dpi=200)\n",
    "x = [0, 500]\n",
    "y = [0, 500]\n",
    "plt.plot(x, y, '--', color='k')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylim(0, 500)\n",
    "plt.plot(Y_cp_train, Y_cp_pred_train, 'o', color='c', label=\"Training set\")\n",
    "plt.plot(Y_cp_test, Y_cp_pred_test, 'o', color='k', label=\"Test set\")\n",
    "plt.xlabel('Expected $C_{p}$ (cal/g.C)', labelpad=12, fontsize=22, fontname='Times New Roman')\n",
    "plt.ylabel('Predicted $C_{p}$ (cal/g.C)', labelpad=12, fontsize=22, fontname='Times New Roman')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "mse_train = mean_squared_error(Y_cp_train, Y_cp_pred_train)\n",
    "mse_test = mean_squared_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MSE on train set:', mse_train)\n",
    "print('MSE on test set:', mse_test)\n",
    "\n",
    "\n",
    "mse_per_sample_train = mse_train / len(Y_cp_train)\n",
    "mse_per_sample_test = mse_test / len(Y_cp_test)\n",
    "\n",
    "print('MSE per sample on train set:', mse_per_sample_train)\n",
    "print('MSE per sample on test set:', mse_per_sample_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mae_train = mean_absolute_error(Y_cp_train, Y_cp_pred_train)\n",
    "mae_test = mean_absolute_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MAE on train set:', mae_train)\n",
    "print('MAE on test set:', mae_test)\n",
    "\n",
    "mae_per_sample_train = mae_train / len(Y_cp_train)\n",
    "mae_per_sample_test = mae_test / len(Y_cp_test)\n",
    "\n",
    "print('MAE per sample on train set:', mae_per_sample_train)\n",
    "print('MAE per sample on test set:', mae_per_sample_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    #loss = tf.keras.losses.Huber(delta)(y_true, y_pred)\n",
    "    loss = Huber(delta)(y_true, y_pred)\n",
    "    return loss.numpy()\n",
    "\n",
    "huber_loss_train = huber_loss(Y_cp_train, Y_cp_pred_train)\n",
    "huber_loss_test = huber_loss(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "\n",
    "print('Huber Loss on train set:', huber_loss_train)\n",
    "print('Huber Loss on test set:', huber_loss_test)\n",
    "\n",
    "\n",
    "\n",
    "def wing_loss(y_true, y_pred, w=5.0, epsilon=1.5):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    w = tf.cast(w, tf.float64)\n",
    "    epsilon = tf.cast(epsilon, tf.float64)\n",
    "    c = w * (1.0 - K.log(1.0 + w/epsilon))\n",
    "    absolute = K.abs(y_true - y_pred)\n",
    "    losses = tf.where(absolute < w, w * K.log(2.0 + absolute/epsilon), absolute - c)\n",
    "    return K.mean(losses).numpy()\n",
    "\n",
    "\n",
    "wing_loss_train = wing_loss(Y_cp_train, Y_cp_pred_train)\n",
    "wing_loss_test = wing_loss(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('Wing Loss on train set:', wing_loss_train)\n",
    "print('Wing Loss on test set:', wing_loss_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train model with MSE loss function to predict Cp\n",
    "\n",
    "# Split the dataset\n",
    "X_cp_train, X_cp_test, Y_cp_train, Y_cp_test = train_test_split(X_cp, Y_cp, test_size=0.35, random_state=70)\n",
    "\n",
    "X_cp_train = np.array(X_cp_train).astype(\"float\")\n",
    "Y_cp_train = np.array(Y_cp_train).astype(\"float\")\n",
    "X_cp_test = np.array(X_cp_test).astype(\"float\")\n",
    "Y_cp_test = np.array(Y_cp_test).astype(\"float\")\n",
    "\n",
    "tf.keras.utils.set_random_seed(20)\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 10)  # Number of hidden layers\n",
    "    units = trial.suggest_int('units', 128, 1024, step=64)  # Number of neurons in each layer\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)  # Dropout rate\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)  # Learning rate\n",
    "    batch_size = trial.suggest_int('batch_size', 10, 100, step=10)  # Batch size\n",
    "    epochs = trial.suggest_int('epochs', 100, 700)  # Epochs\n",
    "\n",
    "    # Model architecture\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units, activation='relu', input_dim=13))\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))  # Dropout\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='linear'))  # Output layer\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_cp_train, Y_cp_train, validation_split=0.3, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Predict on test set\n",
    "    Y_cp_pred_test = model.predict(X_cp_test)\n",
    "    \n",
    "    # Calculate R2 score as the evaluation metric (the higher, the better)\n",
    "    R2test = r2_score(Y_cp_test, Y_cp_pred_test)\n",
    "    \n",
    "    # Since Optuna minimizes the objective function, return negative R2\n",
    "    return -R2test\n",
    "\n",
    "# Create Optuna study and run the optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best trial results\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value:', -trial.value)  # Convert back to positive R2\n",
    "print('  Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "# Plot the training and validation loss over epochs for the best model\n",
    "best_model = keras.Sequential()\n",
    "best_model.add(layers.Dense(trial.params['units'], activation='relu', input_dim=13))\n",
    "\n",
    "for i in range(trial.params['num_layers']):\n",
    "    best_model.add(layers.Dense(trial.params['units'], activation='relu'))\n",
    "    best_model.add(layers.Dropout(trial.params['dropout_rate']))\n",
    "\n",
    "best_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the best model\n",
    "best_model.compile(optimizer=keras.optimizers.Adam(learning_rate=trial.params['learning_rate']), loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the best model\n",
    "history_callback = best_model.fit(X_cp_train, Y_cp_train, validation_split=0.3, epochs=trial.params['epochs'], batch_size=trial.params['batch_size'])\n",
    "\n",
    "# Predict on training and test sets\n",
    "Y_cp_pred_train = best_model.predict(X_cp_train)\n",
    "Y_cp_pred_test = best_model.predict(X_cp_test)\n",
    "\n",
    "# R2 Scores\n",
    "R2train = r2_score(Y_cp_train, Y_cp_pred_train)\n",
    "R2test = r2_score(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('R2 Score on Train set:', R2train)\n",
    "print('R2 Score on Test set:', R2test)\n",
    "\n",
    "# Plot training and validation loss over epochs\n",
    "history_dict = history_callback.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig = plt.figure(dpi=200)\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot actual vs predicted\n",
    "fig = plt.figure(dpi=200)\n",
    "x = [0, 500]\n",
    "y = [0, 500]\n",
    "plt.plot(x, y, '--', color='k')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylim(0, 500)\n",
    "plt.plot(Y_cp_train, Y_cp_pred_train, 'o', color='#ADFF2F', label=\"Training set\")\n",
    "plt.plot(Y_cp_test, Y_cp_pred_test, 'o', color='k', label=\"Test set\")\n",
    "plt.xlabel('Expected $C_{p}$ (cal/g.C)', labelpad=12, fontsize=22, fontname='Times New Roman')\n",
    "plt.ylabel('Predicted $C_{p}$ (cal/g.C)', labelpad=12, fontsize=22, fontname='Times New Roman')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#save the model to use in Transfer learning\n",
    "\n",
    "model.save('Model.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mse_train = mean_squared_error(Y_cp_train, Y_cp_pred_train)\n",
    "mse_test = mean_squared_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MSE on train set:', mse_train)\n",
    "print('MSE on test set:', mse_test)\n",
    "\n",
    "\n",
    "mse_per_sample_train = mse_train / len(Y_cp_train)\n",
    "mse_per_sample_test = mse_test / len(Y_cp_test)\n",
    "\n",
    "print('MSE per sample on train set:', mse_per_sample_train)\n",
    "print('MSE per sample on test set:', mse_per_sample_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mae_train = mean_absolute_error(Y_cp_train, Y_cp_pred_train)\n",
    "mae_test = mean_absolute_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MAE on train set:', mae_train)\n",
    "print('MAE on test set:', mae_test)\n",
    "\n",
    "mae_per_sample_train = mae_train / len(Y_cp_train)\n",
    "mae_per_sample_test = mae_test / len(Y_cp_test)\n",
    "\n",
    "print('MAE per sample on train set:', mae_per_sample_train)\n",
    "print('MAE per sample on test set:', mae_per_sample_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    #loss = tf.keras.losses.Huber(delta)(y_true, y_pred)\n",
    "    loss = Huber(delta)(y_true, y_pred)\n",
    "    return loss.numpy()\n",
    "\n",
    "huber_loss_train = huber_loss(Y_cp_train, Y_cp_pred_train)\n",
    "huber_loss_test = huber_loss(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "\n",
    "print('Huber Loss on train set:', huber_loss_train)\n",
    "print('Huber Loss on test set:', huber_loss_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wing_loss(y_true, y_pred, w=5.0, epsilon=1.5):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    w = tf.cast(w, tf.float64)\n",
    "    epsilon = tf.cast(epsilon, tf.float64)\n",
    "    c = w * (1.0 - K.log(1.0 + w/epsilon))\n",
    "    absolute = K.abs(y_true - y_pred)\n",
    "    losses = tf.where(absolute < w, w * K.log(2.0 + absolute/epsilon), absolute - c)\n",
    "    return K.mean(losses).numpy()\n",
    "\n",
    "\n",
    "wing_loss_train = wing_loss(Y_cp_train, Y_cp_pred_train)\n",
    "wing_loss_test = wing_loss(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('Wing Loss on train set:', wing_loss_train)\n",
    "print('Wing Loss on test set:', wing_loss_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b364555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model with Huber loss function to predict Cp\n",
    "\n",
    "# Split the dataset\n",
    "X_cp_train, X_cp_test, Y_cp_train, Y_cp_test = train_test_split(X_cp, Y_cp, test_size=0.35, random_state=70)\n",
    "\n",
    "X_cp_train = np.array(X_cp_train).astype(\"float\")\n",
    "Y_cp_train = np.array(Y_cp_train).astype(\"float\")\n",
    "X_cp_test = np.array(X_cp_test).astype(\"float\")\n",
    "Y_cp_test = np.array(Y_cp_test).astype(\"float\")\n",
    "\n",
    "tf.keras.utils.set_random_seed(20)\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 10)  # Number of hidden layers\n",
    "    units = trial.suggest_int('units', 128, 1024, step=64)  # Number of neurons in each layer\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)  # Dropout rate\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)  # Learning rate\n",
    "    batch_size = trial.suggest_int('batch_size', 10, 100, step=10)  # Batch size\n",
    "    epochs = trial.suggest_int('epochs', 100, 700)  # Epochs\n",
    "\n",
    "    # Model architecture\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units, activation='relu', input_dim=13))\n",
    "    \n",
    "    for _ in range(num_layers - 1):  # Add the specified number of layers\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))  # Dropout\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='linear'))  # Output layer\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                  loss=tf.keras.losses.Huber(delta=1.0), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_cp_train, Y_cp_train, validation_split=0.3, epochs=epochs, \n",
    "                        batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Predict on test set\n",
    "    Y_cp_pred_test = model.predict(X_cp_test)\n",
    "    \n",
    "    # Calculate R2 score as the evaluation metric (the higher, the better)\n",
    "    R2test = r2_score(Y_cp_test, Y_cp_pred_test)\n",
    "    \n",
    "    # Since Optuna minimizes the objective function, return negative R2\n",
    "    return -R2test\n",
    "\n",
    "# Create Optuna study and run the optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best trial results\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value:', -trial.value)  # Convert back to positive R2\n",
    "print('  Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "# Train the best model with the optimized hyperparameters\n",
    "best_model = keras.Sequential()\n",
    "best_model.add(layers.Dense(trial.params['units'], activation='relu', input_dim=13))\n",
    "\n",
    "for _ in range(trial.params['num_layers'] - 1):\n",
    "    best_model.add(layers.Dense(trial.params['units'], activation='relu'))\n",
    "    best_model.add(layers.Dropout(trial.params['dropout_rate']))\n",
    "\n",
    "best_model.add(layers.Dense(1, activation='linear'))  # Output layer\n",
    "\n",
    "# Compile the best model\n",
    "best_model.compile(optimizer=keras.optimizers.Adam(learning_rate=trial.params['learning_rate']),\n",
    "                   loss=tf.keras.losses.Huber(delta=1.0), \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the best model\n",
    "history_callback = best_model.fit(X_cp_train, Y_cp_train, validation_split=0.3, \n",
    "                                   epochs=trial.params['epochs'], \n",
    "                                   batch_size=trial.params['batch_size'])\n",
    "\n",
    "# Predict on training and test sets\n",
    "Y_cp_pred_train = best_model.predict(X_cp_train)\n",
    "Y_cp_pred_test = best_model.predict(X_cp_test)\n",
    "\n",
    "# R2 Scores\n",
    "R2train = r2_score(Y_cp_train, Y_cp_pred_train)\n",
    "R2test = r2_score(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('R2 Score on Train set:', R2train)\n",
    "print('R2 Score on Test set:', R2test)\n",
    "\n",
    "# Plot training and validation loss over epochs\n",
    "history_dict = history_callback.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig = plt.figure(dpi=200)\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot actual vs predicted\n",
    "fig = plt.figure(dpi=200)\n",
    "x = [0, 500]\n",
    "y = [0, 500]\n",
    "plt.plot(x, y, '--', color='k')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylim(0, 500)\n",
    "plt.plot(Y_cp_train, Y_cp_pred_train, 'o', color='#ADFF2F', label=\"Training set\")\n",
    "plt.plot(Y_cp_test, Y_cp_pred_test, 'o', color='k', label=\"Test set\")\n",
    "plt.xlabel('Expected $C_{p}$ (cal/g.C)', labelpad=12, fontsize=22, fontname='Times New Roman')\n",
    "plt.ylabel('Predicted $C_{p}$ (cal/g.C)', labelpad=12, fontsize=22, fontname='Times New Roman')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "mse_train = mean_squared_error(Y_cp_train, Y_cp_pred_train)\n",
    "mse_test = mean_squared_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MSE on train set:', mse_train)\n",
    "print('MSE on test set:', mse_test)\n",
    "\n",
    "\n",
    "mse_per_sample_train = mse_train / len(Y_cp_train)\n",
    "mse_per_sample_test = mse_test / len(Y_cp_test)\n",
    "\n",
    "print('MSE per sample on train set:', mse_per_sample_train)\n",
    "print('MSE per sample on test set:', mse_per_sample_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mae_train = mean_absolute_error(Y_cp_train, Y_cp_pred_train)\n",
    "mae_test = mean_absolute_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MAE on train set:', mae_train)\n",
    "print('MAE on test set:', mae_test)\n",
    "\n",
    "mae_per_sample_train = mae_train / len(Y_cp_train)\n",
    "mae_per_sample_test = mae_test / len(Y_cp_test)\n",
    "\n",
    "print('MAE per sample on train set:', mae_per_sample_train)\n",
    "print('MAE per sample on test set:', mae_per_sample_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    #loss = tf.keras.losses.Huber(delta)(y_true, y_pred)\n",
    "    loss = Huber(delta)(y_true, y_pred)\n",
    "    return loss.numpy()\n",
    "\n",
    "huber_loss_train = huber_loss(Y_cp_train, Y_cp_pred_train)\n",
    "huber_loss_test = huber_loss(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "\n",
    "print('Huber Loss on train set:', huber_loss_train)\n",
    "print('Huber Loss on test set:', huber_loss_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wing_loss(y_true, y_pred, w=5.0, epsilon=1.5):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    w = tf.cast(w, tf.float64)\n",
    "    epsilon = tf.cast(epsilon, tf.float64)\n",
    "    c = w * (1.0 - K.log(1.0 + w/epsilon))\n",
    "    absolute = K.abs(y_true - y_pred)\n",
    "    losses = tf.where(absolute < w, w * K.log(2.0 + absolute/epsilon), absolute - c)\n",
    "    return K.mean(losses).numpy()\n",
    "\n",
    "\n",
    "wing_loss_train = wing_loss(Y_cp_train, Y_cp_pred_train)\n",
    "wing_loss_test = wing_loss(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('Wing Loss on train set:', wing_loss_train)\n",
    "print('Wing Loss on test set:', wing_loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04860bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model with wing shape function to predict Cp\n",
    "\n",
    "\n",
    "# Split the dataset (ensure X_cp and Y_cp are defined)\n",
    "X_cp_train, X_cp_test, Y_cp_train, Y_cp_test = train_test_split(X_cp, Y_cp, test_size=0.35, random_state=70)\n",
    "\n",
    "X_cp_train = np.array(X_cp_train).astype(\"float\")\n",
    "Y_cp_train = np.array(Y_cp_train).astype(\"float\")\n",
    "X_cp_test = np.array(X_cp_test).astype(\"float\")\n",
    "Y_cp_test = np.array(Y_cp_test).astype(\"float\")\n",
    "\n",
    "tf.keras.utils.set_random_seed(20)\n",
    "\n",
    "# Define Huber loss function\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    loss = tf.keras.losses.Huber(delta)(y_true, y_pred)\n",
    "    return loss.numpy()\n",
    "\n",
    "# Define Wing loss function\n",
    "def wing_loss(y_true, y_pred, w=5.0, epsilon=1.5):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    w = tf.cast(w, tf.float64)\n",
    "    epsilon = tf.cast(epsilon, tf.float64)\n",
    "    c = w * (1.0 - tf.math.log(1.0 + w/epsilon))\n",
    "    absolute = tf.abs(y_true - y_pred)\n",
    "    losses = tf.where(absolute < w, w * tf.math.log(2.0 + absolute/epsilon), absolute - c)\n",
    "    return tf.reduce_mean(losses).numpy()\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 10)  # Number of hidden layers\n",
    "    units = trial.suggest_int('units', 128, 1024, step=64)  # Number of neurons in each layer\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)  # Dropout rate\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)  # Learning rate\n",
    "    batch_size = trial.suggest_int('batch_size', 10, 100, step=10)  # Batch size\n",
    "    epochs = trial.suggest_int('epochs', 100, 700)  # Epochs\n",
    "\n",
    "    # Model architecture\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units, activation='relu', input_dim=13))\n",
    "    \n",
    "    for _ in range(num_layers - 1):  # Add the specified number of layers\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))  # Dropout\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='linear'))  # Output layer\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                  loss=wing_loss, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_cp_train, Y_cp_train, validation_split=0.3, epochs=epochs, \n",
    "              batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Predict on test set\n",
    "    Y_cp_pred_test = model.predict(X_cp_test)\n",
    "    \n",
    "    # Calculate R2 score as the evaluation metric (the higher, the better)\n",
    "    R2test = r2_score(Y_cp_test, Y_cp_pred_test)\n",
    "    \n",
    "    # Since Optuna minimizes the objective function, return negative R2\n",
    "    return -R2test\n",
    "\n",
    "# Create Optuna study and run the optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best trial results\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value:', -trial.value)  # Convert back to positive R2\n",
    "print('  Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "# Train the best model with the optimized hyperparameters\n",
    "best_model = keras.Sequential()\n",
    "best_model.add(layers.Dense(trial.params['units'], activation='relu', input_dim=13))\n",
    "\n",
    "for _ in range(trial.params['num_layers'] - 1):\n",
    "    best_model.add(layers.Dense(trial.params['units'], activation='relu'))\n",
    "    best_model.add(layers.Dropout(trial.params['dropout_rate']))\n",
    "\n",
    "best_model.add(layers.Dense(1, activation='linear'))  # Output layer\n",
    "\n",
    "# Compile the best model\n",
    "best_model.compile(optimizer=keras.optimizers.Adam(learning_rate=trial.params['learning_rate']),\n",
    "                   loss=wing_loss, \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the best model\n",
    "history_callback = best_model.fit(X_cp_train, Y_cp_train, validation_split=0.3, \n",
    "                                   epochs=trial.params['epochs'], \n",
    "                                   batch_size=trial.params['batch_size'])\n",
    "\n",
    "# Predict on training and test sets\n",
    "Y_cp_pred_train = best_model.predict(X_cp_train)\n",
    "Y_cp_pred_test = best_model.predict(X_cp_test)\n",
    "\n",
    "# R2 Scores\n",
    "R2train = r2_score(Y_cp_train, Y_cp_pred_train)\n",
    "R2test = r2_score(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('R2 Score on Train set:', R2train)\n",
    "print('R2 Score on Test set:', R2test)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse_train = mean_squared_error(Y_cp_train, Y_cp_pred_train)\n",
    "mse_test = mean_squared_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MSE on train set:', mse_train)\n",
    "print('MSE on test set:', mse_test)\n",
    "\n",
    "mse_per_sample_train = mse_train / len(Y_cp_train)\n",
    "mse_per_sample_test = mse_test / len(Y_cp_test)\n",
    "\n",
    "print('MSE per sample on train set:', mse_per_sample_train)\n",
    "print('MSE per sample on test set:', mse_per_sample_test)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae_train = mean_absolute_error(Y_cp_train, Y_cp_pred_train)\n",
    "mae_test = mean_absolute_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MAE on train set:', mae_train)\n",
    "print('MAE on test set:', mae_test)\n",
    "\n",
    "mae_per_sample_train = mae_train / len(Y_cp_train)\n",
    "mae_per_sample_test = mae_test / len(Y_cp_test)\n",
    "\n",
    "print('MAE per sample on train set:', mae_per_sample_train)\n",
    "print('MAE per sample on test set:', mae_per_sample_test)\n",
    "\n",
    "# Plot training and validation loss over epochs\n",
    "history_dict = history_callback.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig = plt.figure(dpi=200)\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot actual vs predicted\n",
    "fig = plt.figure(dpi=200)\n",
    "x = [0, 500]\n",
    "y = [0, 500]\n",
    "plt.plot(x, y, '--', color='k')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylim(0, 500)\n",
    "plt.plot(Y_cp_train, Y_cp_pred_train, 'o', color='#ADFF2F', label=\"Training set\")\n",
    "plt.plot(Y_cp_test, Y_cp_pred_test, 'o', color='k', label=\"Test set\")\n",
    "plt.xlabel('Expected $C_{p}$ (cal/g.C)', labelpad=12, fontsize=22, fontname='Times New Roman')\n",
    "plt.ylabel('Predicted $C_{p}$ (cal/g.C)', labelpad=12, fontsize=22, fontname='Times New Roman')\n",
    "plt.legend(loc='best', fontsize=7, prop={'family': 'Times New Roman'})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mse_train = mean_squared_error(Y_cp_train, Y_cp_pred_train)\n",
    "mse_test = mean_squared_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MSE on train set:', mse_train)\n",
    "print('MSE on test set:', mse_test)\n",
    "\n",
    "\n",
    "mse_per_sample_train = mse_train / len(Y_cp_train)\n",
    "mse_per_sample_test = mse_test / len(Y_cp_test)\n",
    "\n",
    "print('MSE per sample on train set:', mse_per_sample_train)\n",
    "print('MSE per sample on test set:', mse_per_sample_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mae_train = mean_absolute_error(Y_cp_train, Y_cp_pred_train)\n",
    "mae_test = mean_absolute_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MAE on train set:', mae_train)\n",
    "print('MAE on test set:', mae_test)\n",
    "\n",
    "mae_per_sample_train = mae_train / len(Y_cp_train)\n",
    "mae_per_sample_test = mae_test / len(Y_cp_test)\n",
    "\n",
    "print('MAE per sample on train set:', mae_per_sample_train)\n",
    "print('MAE per sample on test set:', mae_per_sample_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    #loss = tf.keras.losses.Huber(delta)(y_true, y_pred)\n",
    "    loss = Huber(delta)(y_true, y_pred)\n",
    "    return loss.numpy()\n",
    "\n",
    "huber_loss_train = huber_loss(Y_cp_train, Y_cp_pred_train)\n",
    "huber_loss_test = huber_loss(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "\n",
    "print('Huber Loss on train set:', huber_loss_train)\n",
    "print('Huber Loss on test set:', huber_loss_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wing_loss(y_true, y_pred, w=5.0, epsilon=1.5):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    w = tf.cast(w, tf.float64)\n",
    "    epsilon = tf.cast(epsilon, tf.float64)\n",
    "    c = w * (1.0 - K.log(1.0 + w/epsilon))\n",
    "    absolute = K.abs(y_true - y_pred)\n",
    "    losses = tf.where(absolute < w, w * K.log(2.0 + absolute/epsilon), absolute - c)\n",
    "    return K.mean(losses).numpy()\n",
    "\n",
    "\n",
    "wing_loss_train = wing_loss(Y_cp_train, Y_cp_pred_train)\n",
    "wing_loss_test = wing_loss(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('Wing Loss on train set:', wing_loss_train)\n",
    "print('Wing Loss on test set:', wing_loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb90aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model with combined (MSE,MAE, wing shape, and Huber)  loss function to predict Cp\n",
    "# Split the dataset (ensure X_cp and Y_cp are defined)\n",
    "X_cp_train, X_cp_test, Y_cp_train, Y_cp_test = train_test_split(X_cp, Y_cp, test_size=0.35, random_state=70)\n",
    "\n",
    "X_cp_train = np.array(X_cp_train).astype(\"float\")\n",
    "Y_cp_train = np.array(Y_cp_train).astype(\"float\")\n",
    "X_cp_test = np.array(X_cp_test).astype(\"float\")\n",
    "Y_cp_test = np.array(Y_cp_test).astype(\"float\")\n",
    "\n",
    "tf.keras.utils.set_random_seed(20)\n",
    "\n",
    "# Define the custom combined loss function\n",
    "def combined_loss(y_true, y_pred):\n",
    "    # Define the parameters for the loss functions\n",
    "    delta = 1\n",
    "    epsilon = 1.5\n",
    "    w = 5\n",
    "\n",
    "    # Compute the Mean Squared Error loss\n",
    "    mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    # Compute the Mean Absolute Error loss\n",
    "    mae_loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # Compute the Huber loss\n",
    "    huber_loss = tf.keras.losses.Huber(delta)(y_true, y_pred)\n",
    "\n",
    "    # Compute the Wing loss\n",
    "    residuals = tf.abs(y_true - y_pred)\n",
    "    wing_loss = tf.reduce_mean(\n",
    "        tf.where(residuals < w,\n",
    "                 w * tf.math.log(1.0 + (residuals / w)),\n",
    "                 residuals - (w * (tf.math.log(1.0 + (w / epsilon))))\n",
    "                )\n",
    "    )\n",
    "\n",
    "    # Combine the losses using a weighted sum\n",
    "    loss = (0.25 * mse_loss) + (0.25 * huber_loss) + (0.25 * wing_loss) + (0.25 * mae_loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Define the model creation function\n",
    "def create_model(trial):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Hyperparameters to optimize\n",
    "    model.add(layers.Dense(trial.suggest_int('units_1', 128, 1024), activation='relu', input_dim=13))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    for i in range(trial.suggest_int('n_layers', 1, 5)):\n",
    "        model.add(layers.Dense(trial.suggest_int(f'units_{i+2}', 128, 1024), activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='linear'))  # Output layer\n",
    "\n",
    "    # Suggest a learning rate\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=combined_loss, metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "  \n",
    "    # Create model\n",
    "    model = create_model(trial)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_cp_train, Y_cp_train, validation_split=0.3, \n",
    "                        epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    Y_cp_pred_test = model.predict(X_cp_test)\n",
    "    r2 = r2_score(Y_cp_test, Y_cp_pred_test)\n",
    "    return r2  # Return R^2 score for optimization\n",
    "\n",
    "# Create a study object and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # Change the number of trials as needed\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# Optionally, you can train the model with the best parameters and visualize the results\n",
    "best_model = create_model(study.best_trial)\n",
    "best_model.fit(X_cp_train, Y_cp_train, validation_split=0.3, epochs=100, batch_size=10, verbose=1)\n",
    "\n",
    "# Predict and plot results\n",
    "Y_cp_pred_train = best_model.predict(X_cp_train)\n",
    "Y_cp_pred_test = best_model.predict(X_cp_test)\n",
    "\n",
    "# Calculate R² scores\n",
    "R2train = r2_score(Y_cp_train, Y_cp_pred_train)\n",
    "R2test = r2_score(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('R² score on training set:', R2train)\n",
    "print('R² score on test set:', R2test)\n",
    "\n",
    "# Plotting actual vs predicted values\n",
    "fig = plt.figure(dpi=200)\n",
    "x = [0, 500]\n",
    "y = [0, 500]\n",
    "plt.plot(x, y, '--', color='k')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylim(0, 500)\n",
    "plt.plot(Y_cp_train, Y_cp_pred_train, 'o', color='c', label=\"Training set\")\n",
    "plt.plot(Y_cp_test, Y_cp_pred_test, 'o', color='k', label=\"Test set\")\n",
    "plt.xlabel('Expected $C_{p}$ (cal/g.C)', labelpad=12, fontsize=22, fontname='Times New Roman')\n",
    "plt.ylabel('Predicted $C_{p}$ (cal/g.C)', labelpad=12, fontsize=22, fontname='Times New Roman')\n",
    "plt.legend(loc='best', fontsize=7, prop={'family': 'Times New Roman'})\n",
    "plt.title('Expected vs Predicted $C_{p}$', fontsize=22)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "mse_train = mean_squared_error(Y_cp_train, Y_cp_pred_train)\n",
    "mse_test = mean_squared_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MSE on train set:', mse_train)\n",
    "print('MSE on test set:', mse_test)\n",
    "\n",
    "\n",
    "mse_per_sample_train = mse_train / len(Y_cp_train)\n",
    "mse_per_sample_test = mse_test / len(Y_cp_test)\n",
    "\n",
    "print('MSE per sample on train set:', mse_per_sample_train)\n",
    "print('MSE per sample on test set:', mse_per_sample_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mae_train = mean_absolute_error(Y_cp_train, Y_cp_pred_train)\n",
    "mae_test = mean_absolute_error(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('MAE on train set:', mae_train)\n",
    "print('MAE on test set:', mae_test)\n",
    "\n",
    "mae_per_sample_train = mae_train / len(Y_cp_train)\n",
    "mae_per_sample_test = mae_test / len(Y_cp_test)\n",
    "\n",
    "print('MAE per sample on train set:', mae_per_sample_train)\n",
    "print('MAE per sample on test set:', mae_per_sample_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    #loss = tf.keras.losses.Huber(delta)(y_true, y_pred)\n",
    "    loss = Huber(delta)(y_true, y_pred)\n",
    "    return loss.numpy()\n",
    "\n",
    "huber_loss_train = huber_loss(Y_cp_train, Y_cp_pred_train)\n",
    "huber_loss_test = huber_loss(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "\n",
    "print('Huber Loss on train set:', huber_loss_train)\n",
    "print('Huber Loss on test set:', huber_loss_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wing_loss(y_true, y_pred, w=5.0, epsilon=1.5):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    w = tf.cast(w, tf.float64)\n",
    "    epsilon = tf.cast(epsilon, tf.float64)\n",
    "    c = w * (1.0 - K.log(1.0 + w/epsilon))\n",
    "    absolute = K.abs(y_true - y_pred)\n",
    "    losses = tf.where(absolute < w, w * K.log(2.0 + absolute/epsilon), absolute - c)\n",
    "    return K.mean(losses).numpy()\n",
    "\n",
    "\n",
    "wing_loss_train = wing_loss(Y_cp_train, Y_cp_pred_train)\n",
    "wing_loss_test = wing_loss(Y_cp_test, Y_cp_pred_test)\n",
    "\n",
    "print('Wing Loss on train set:', wing_loss_train)\n",
    "print('Wing Loss on test set:', wing_loss_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f90ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
